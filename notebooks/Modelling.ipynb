{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ada2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, learning_curve, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "from housing_crawler.analysis.ads_table_processing import get_processed_ads_table\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa2498",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3fe429",
   "metadata": {},
   "source": [
    "## Obtain data for WGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6732d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlo/code/chvieira2/housing_crawler/housing_crawler/utils.py:35: DtypeWarning: Columns (22,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(local_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loaded all_encoded.csv locally\n",
      "===> ads_OSM.csv saved locally\n"
     ]
    }
   ],
   "source": [
    "df_original = get_processed_ads_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150d1e8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ads_feat_df = df_original[df_original['details_searched']==1]\n",
    "# ads_feat_df = ads_feat_df[ads_feat_df['city'].isin(['Berlin', 'München', 'Hamburg', 'Stuttgart', 'Köln', 'Münster', 'Leipzig', 'Frankfurt am Main'])]\n",
    "ads_feat_df = ads_feat_df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ece333",
   "metadata": {},
   "source": [
    "## Remove duplicates if exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f455f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of duplicate data points\n",
    "# It's very likely zero cause I already removed dulicated IDs during processing\n",
    "ads_feat_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04656a",
   "metadata": {},
   "source": [
    "## Filter data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5b4f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6465, 142)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only ads that have been searched for details (search added from august on)\n",
    "df_filtered = ads_feat_df.copy()\n",
    "df_filtered = df_filtered[df_filtered['type_offer_simple']=='WG']\n",
    "df_filtered = df_filtered[df_filtered['km_to_centroid'].notna()]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e4d9b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drop columns I won't use for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491c389",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_filtered = df_filtered[[\n",
    "#         'url',\n",
    "        'commercial_landlord',\n",
    "#         'title',\n",
    "#         'type_offer_simple',\n",
    "        'size_sqm',\n",
    "#         'home_total_size',\n",
    "#         'available_rooms',\n",
    "        'capacity',\n",
    "#         'available_spots_wg',\n",
    "    \n",
    "#         'address',\n",
    "        'city',\n",
    "#         'zip_code',\n",
    "#         'latitude',\n",
    "#         'longitude',\n",
    "    \n",
    "#         'published_on',\n",
    "#         'published_at',\n",
    "#         'day_of_week_publication',\n",
    "#         'available_from',\n",
    "#         'available_to',\n",
    "        'days_available',\n",
    "        'rental_length_term',\n",
    "        'sin_published_at',\n",
    "        'cos_published_at',\n",
    "        'sin_day_week_int',\n",
    "        'cos_day_week_int',\n",
    "    \n",
    "#         'details_searched',\n",
    "#         'crawler',\n",
    "    \n",
    "# Values\n",
    "        'price_euros',    \n",
    "#         'cold_rent_euros',\n",
    "#         'mandatory_costs_euros',\n",
    "#         'extra_costs_euros',\n",
    "#         'deposit',\n",
    "        'transfer_costs_euros',\n",
    "#         'price_per_sqm',\n",
    "\n",
    "# Flatmates\n",
    "        'male_flatmates',\n",
    "        'female_flatmates',\n",
    "        'diverse_flatmates',\n",
    "        'min_age_flatmates',\n",
    "        'max_age_flatmates',\n",
    "    \n",
    "# Person searched\n",
    "        'gender_searched',\n",
    "        'min_age_searched',\n",
    "        'max_age_searched',\n",
    "        'age_category_searched',\n",
    "    \n",
    "# Details\n",
    "    \n",
    "        'schufa_needed',\n",
    "#         'wg_possible',\n",
    "    \n",
    "#         'smoking',\n",
    "        'smoking_numerical',\n",
    "        'building_type',\n",
    "        'building_floor',\n",
    "#         'furniture',\n",
    "        'furniture_numerical',\n",
    "#         'kitchen',\n",
    "        'kitchen_numerical',\n",
    "        'heating',\n",
    "        'public_transport_distance',\n",
    "        'parking',\n",
    "    \n",
    "#         'construction_year',\n",
    "#         'energy_certificate',\n",
    "#         'energy_usage',\n",
    "#         'energy_efficiency_class',\n",
    "#         'heating_energy_source',\n",
    "    \n",
    "        'tv_kabel',\n",
    "        'tv_satellit',\n",
    "    \n",
    "#         'toilet',\n",
    "        'shower_type_badewanne',\n",
    "        'shower_type_dusche',\n",
    "    \n",
    "        'floor_type_dielen',\n",
    "        'floor_type_parkett',\n",
    "        'floor_type_laminat',\n",
    "        'floor_type_teppich',\n",
    "        'floor_type_fliesen',\n",
    "        'floor_type_pvc',\n",
    "        'floor_type_fußbodenheizung',\n",
    "    \n",
    "        'extras_waschmaschine',\n",
    "        'extras_spuelmaschine',\n",
    "        'extras_terrasse',\n",
    "        'extras_balkon',\n",
    "        'extras_garten',\n",
    "        'extras_gartenmitbenutzung',\n",
    "        'extras_keller',\n",
    "        'extras_aufzug',\n",
    "        'extras_haustiere',\n",
    "        'extras_fahrradkeller',\n",
    "        'extras_dachboden',\n",
    "    \n",
    "# WG only\n",
    "        'number_languages',\n",
    "        'languages_deutsch',\n",
    "        'languages_englisch',\n",
    "    \n",
    "        'wg_type_studenten',\n",
    "        'wg_type_keine_zweck',\n",
    "        'wg_type_maenner',\n",
    "        'wg_type_business',\n",
    "        'wg_type_wohnheim',\n",
    "        'wg_type_vegetarisch_vegan',\n",
    "        'wg_type_alleinerziehende',\n",
    "        'wg_type_funktionale',\n",
    "        'wg_type_berufstaetigen',\n",
    "        'wg_type_gemischte',\n",
    "        'wg_type_mit_kindern',\n",
    "        'wg_type_verbindung',\n",
    "        'wg_type_lgbtqia',\n",
    "        'wg_type_senioren',\n",
    "        'wg_type_inklusive',\n",
    "        'wg_type_wg_neugruendung',\n",
    "    \n",
    "        'internet_speed',\n",
    "        'internet_dsl',\n",
    "        'internet_wlan',\n",
    "        'internet_flatrate',\n",
    "    \n",
    "\n",
    "# Geographical\n",
    "        'km_to_centroid',\n",
    "        'sin_degrees_to_centroid',\n",
    "        'cos_degrees_to_centroid',\n",
    "\n",
    "# OSM features\n",
    "        'comfort_leisure_spots',\n",
    "        'comfort_warehouse',\n",
    "        'activities_education',\n",
    "        'mobility_public_transport_bus',\n",
    "        'activities_economic',\n",
    "        'comfort_industrial',\n",
    "        'activities_goverment',\n",
    "        'social_life_eating',\n",
    "        'comfort_comfort_spots',\n",
    "        'social_life_culture',\n",
    "        'activities_supermarket',\n",
    "#         'activities_public_service',\n",
    "        'social_life_community',\n",
    "        'comfort_leisure_mass',\n",
    "        'activities_educational',\n",
    "        'mobility_street_secondary',\n",
    "        'mobility_public_transport_rail',\n",
    "        'activities_retail',\n",
    "        'social_life_night_life',\n",
    "        'comfort_green_natural',\n",
    "        'comfort_railway',\n",
    "        'mobility_bike_infraestructure',\n",
    "#         'comfort_green_forests',\n",
    "        'mobility_street_primary',\n",
    "        'comfort_lakes',\n",
    "#         'activities_health_regional',\n",
    "        'activities_health_local',\n",
    "        'comfort_green_space',\n",
    "        'comfort_rivers',\n",
    "        'activities_post',\n",
    "        'comfort_green_parks',\n",
    "        'comfort_street_motorway'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09575a58",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92eb7e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(df_filtered)\n",
    "categorical_columns = categorical_columns_selector(df_filtered)\n",
    "# categorical_columns.remove('city')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e01a28",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196c74a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### List of columns with manually inserted values that would possibly contain outliers and/werid values\n",
    "\n",
    "manually_inserted_values_columns = [\n",
    "    'price_euros',\n",
    "    'size_sqm',\n",
    "#     'cold_rent_euros',\n",
    "#     'mandatory_costs_euros',\n",
    "#     'extra_costs_euros',\n",
    "#     'deposit',\n",
    "#     'home_total_size',\n",
    "    'public_transport_distance',\n",
    "    'transfer_costs_euros',\n",
    "    'min_age_flatmates',\n",
    "    'max_age_flatmates',\n",
    "    'min_age_searched',\n",
    "    'max_age_searched',\n",
    "#     'construction_year',\n",
    "#     'energy_usage'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2f00a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Manually look into the variable distribution to identify outliers\n",
    "col = manually_inserted_values_columns[5]\n",
    "df_filtered[col].hist(bins = 100);\n",
    "# df_filtered[[col]].boxplot()\n",
    "# plt.xlim(0,300)\n",
    "plt.ylim(0,100);\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b596c85",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#### Manage identified outliers\n",
    "# max_age_flatmates\n",
    "# There are some ads with really extreme energy usage values (<18 or >80). These were removed for modelling\n",
    "df_filtered['max_age_flatmates'] = [np.nan if (value!=value) or value < 18 or value >80 else value for value in list(df_filtered['max_age_flatmates'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e28fb5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dealing with NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fb559",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_imputted = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51217d16",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Percentage missing values per column before imputting\n",
    "(df_imputted.isnull().sum().sort_values(ascending=False)/len(df_imputted)*100)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80306c82",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Imputting numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066247c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols_imputing = ['internet_speed',\n",
    "                     'max_age_flatmates','min_age_flatmates',\n",
    "                    'cos_published_at', 'sin_published_at',\n",
    "                    'public_transport_distance','building_floor']\n",
    "\n",
    "col = num_cols_imputing[0]\n",
    "df_imputted[col].hist(bins = 100);\n",
    "# df_filtered[[col]].boxplot()\n",
    "# plt.xlim(0,300)\n",
    "# plt.ylim(0,100);\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6276f24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_cols_imputing_mean = ['min_age_flatmates', 'max_age_flatmates']\n",
    "num_cols_imputing_median = ['internet_speed', 'cos_published_at', 'sin_published_at','public_transport_distance', 'building_floor'] \n",
    "\n",
    "num_imputer_mean = SimpleImputer(strategy=\"mean\") # Instantiate a SimpleImputer object with your strategy of choice\n",
    "num_imputer_median = SimpleImputer(strategy=\"median\") # Instantiate a SimpleImputer object with your strategy of choice\n",
    "\n",
    "num_imputer_mean.fit(df_imputted[num_cols_imputing_mean]) # Call the \"fit\" method on the object\n",
    "num_imputer_median.fit(df_imputted[num_cols_imputing_median]) # Call the \"fit\" method on the object\n",
    "\n",
    "df_imputted[num_cols_imputing_mean] = num_imputer_mean.transform(df_imputted[num_cols_imputing_mean]) # Call the \"transform\" method on the object\n",
    "df_imputted[num_cols_imputing_median] = num_imputer_median.transform(df_imputted[num_cols_imputing_median]) # Call the \"transform\" method on the object\n",
    "\n",
    "num_imputer_mean.statistics_,  num_imputer_median.statistics_# The mean is stored in the transformer's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73028e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Percentage missing values per column after imputing\n",
    "(df_imputted.isnull().sum().sort_values(ascending=False)/len(df_imputted)*100)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d61390",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Imputting categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecb544",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For some unknown reason imputting for transfer_costs_euros doesn't work, so I manually imput 0 values here for transfer_costs_euros without a answer\n",
    "features_noanswer = ['transfer_costs_euros']\n",
    "\n",
    "noanswer_imputer = SimpleImputer(strategy=\"constant\", fill_value=0) \n",
    "\n",
    "noanswer_imputer.fit(df_imputted[features_noanswer])\n",
    "\n",
    "df_imputted[features_noanswer] = noanswer_imputer.transform(df_imputted[features_noanswer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f7c15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Features in which NaN values represent something\n",
    "features_noanswer = ['heating', 'parking', 'building_type']\n",
    "\n",
    "noanswer_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"no_answer\") \n",
    "\n",
    "noanswer_imputer.fit(df_imputted[features_noanswer])\n",
    "\n",
    "df_imputted[features_noanswer] = noanswer_imputer.transform(df_imputted[features_noanswer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecb315",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Percentage missing values per column after imputing\n",
    "(df_imputted.isnull().sum().sort_values(ascending=False)/len(df_imputted)*100)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cace51b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scaling columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc0e6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_scaled = df_imputted.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbe691",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Find colums for each type of scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac7d29",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "features_OSM = [\n",
    "                'comfort_leisure_spots',\n",
    "                'comfort_warehouse',\n",
    "                'activities_education',\n",
    "                'mobility_public_transport_bus',\n",
    "                'activities_economic',\n",
    "                'comfort_industrial',\n",
    "                'activities_goverment',\n",
    "                'social_life_eating',\n",
    "                'comfort_comfort_spots',\n",
    "                'social_life_culture',\n",
    "                'activities_supermarket',\n",
    "#                 'activities_public_service',\n",
    "                'social_life_community',\n",
    "                'comfort_leisure_mass',\n",
    "                'activities_educational',\n",
    "                'mobility_street_secondary',\n",
    "                'mobility_public_transport_rail',\n",
    "                'activities_retail',\n",
    "                'social_life_night_life',\n",
    "                'comfort_green_natural',\n",
    "                'comfort_railway',\n",
    "                'mobility_bike_infraestructure',\n",
    "#                 'comfort_green_forests',\n",
    "                'mobility_street_primary',\n",
    "                'comfort_lakes',\n",
    "#                 'activities_health_regional',\n",
    "                'activities_health_local',\n",
    "                'comfort_green_space',\n",
    "                'comfort_rivers',\n",
    "                'activities_post',\n",
    "                'comfort_green_parks',\n",
    "                'comfort_street_motorway']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a7668",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "continuous_num_cols = ['size_sqm', 'public_transport_distance', 'km_to_centroid'] #+ features_OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98447708",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for numerical_feature in continuous_num_cols:\n",
    "#     fig, ax = plt.subplots(1,3, figsize = (15,5))\n",
    "    \n",
    "#     ax[0].set_title(f\"Distribution of {numerical_feature}\")\n",
    "#     sns.histplot(x = df_scaled[numerical_feature], kde = True, ax = ax[0])\n",
    "    \n",
    "#     ax[1].set_title(f\"Boxplot of {numerical_feature}\")\n",
    "#     sns.boxplot(x = df_scaled[numerical_feature], ax = ax[1])\n",
    "    \n",
    "#     ax[2].set_title(f\"QQplot of {numerical_feature}\")\n",
    "#     qqplot(df_scaled[numerical_feature], line='s', ax = ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb658de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = continuous_num_cols#numerical_columns[0:3]\n",
    "sns.pairplot(df_scaled.reset_index(), vars=cols);\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed019912",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_minmax_scaler = ['capacity', 'sin_published_at', 'cos_published_at', 'transfer_costs_euros', \n",
    "                      'sin_day_week_int', 'cos_day_week_int', 'male_flatmates', 'female_flatmates', 'diverse_flatmates',\n",
    "                      'min_age_flatmates', 'max_age_flatmates', 'min_age_searched', 'max_age_searched',\n",
    "                      'smoking_numerical', 'building_floor', 'furniture_numerical', 'kitchen_numerical',\n",
    "                      'number_languages', 'internet_speed', 'sin_degrees_to_centroid', 'cos_degrees_to_centroid'] + features_OSM\n",
    "cols_standard_scaler = ['size_sqm', 'km_to_centroid']\n",
    "cols_robust_scaler = ['public_transport_distance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52846c58",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa859e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instanciate MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit scaler to data\n",
    "minmax_scaler.fit(df_scaled[cols_minmax_scaler])\n",
    "standard_scaler.fit(df_scaled[cols_standard_scaler])\n",
    "robust_scaler.fit(df_scaled[cols_robust_scaler])\n",
    "\n",
    "# Use scaler to transform data\n",
    "df_scaled[cols_minmax_scaler] = minmax_scaler.transform(df_scaled[cols_minmax_scaler])\n",
    "df_scaled[cols_standard_scaler] = standard_scaler.transform(df_scaled[cols_standard_scaler])\n",
    "df_scaled[cols_robust_scaler] = robust_scaler.transform(df_scaled[cols_robust_scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd7942",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eab2ca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# OneHot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d7316",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False, drop='if_binary', categories='auto')\n",
    "feature_arr = ohe.fit_transform(df_filtered[categorical_columns])\n",
    "\n",
    "## Get name of new columns and create new dataframe\n",
    "feature_labels = ohe.get_feature_names_out()\n",
    "feature_labels = np.array(feature_labels).ravel()\n",
    "features = pd.DataFrame(feature_arr, columns=feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20057d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Give correct indexes to feature table. Needed for concatenating\n",
    "features.index = df_filtered.index\n",
    "## Add new columns to dataframe\n",
    "df_processed = pd.concat([df_scaled, features], axis = 1).drop(columns=categorical_columns)\n",
    "df_processed.columns = [col.lower().replace(' ', '_').replace('ä','ae').replace('ö','oe').replace('ü','ue').replace('ß','ss') for col in df_processed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3c5aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aa566",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Minimize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f83f9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_minimal = df_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5bb099",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_minimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b3f94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define columns to be tested. Don't test the target, commercial_landlord and 'city'\n",
    "cols_to_search = [col for col in df_minimal.columns if col not in ['price_euros', 'commercial_landlord']]\n",
    "cols_to_search = [col for col in cols_to_search if not col.startswith('city_')]\n",
    "\n",
    "cols_exclude = []\n",
    "for col in cols_to_search:\n",
    "    # How many times the most frequent val exists\n",
    "    most_freq_count = list(df_minimal[col].value_counts())[0]\n",
    "    \n",
    "    if most_freq_count > len(df_minimal)*0.99:\n",
    "        cols_exclude.append(col)\n",
    "\n",
    "        \n",
    "# Exclude all columns (except cities) with >99% of the same value (0) as it contains very little information\n",
    "df_minimal = df_minimal.drop(columns=cols_exclude)\n",
    "df_minimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a8b48a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667521f",
   "metadata": {},
   "source": [
    "# Colinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysed = df_minimal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad17dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style = \"whitegrid\", font_scale= 1)\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))\n",
    "\n",
    "data_corr = df_minimal.corr()\n",
    "sns.heatmap(data_corr, cmap='coolwarm', \n",
    "            annot = False, \n",
    "            annot_kws={\"size\": 8},\n",
    "            vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysed = df_analysed.drop(columns=['age_category_searched_20_100',\n",
    "                                        'extras_gartenmitbenutzung', 'gender_searched_egal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d52db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_corr = df_analysed.corr()\n",
    "\n",
    "corr_df = data_corr.unstack().reset_index() # Unstack correlation matrix \n",
    "corr_df.columns = ['feature_1','feature_2', 'correlation'] # rename columns\n",
    "# corr_df['correlation'] = -corr_df['correlation'] # Invert signal to see negative correlation\n",
    "corr_df.sort_values(by=\"correlation\",ascending=False, inplace=True) # sort by correlation\n",
    "corr_df = corr_df[corr_df['feature_1'] != corr_df['feature_2']] # Remove self correlation\n",
    "corr_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are columns that are always removed, so I'm removing them prematurely to reduce the calculation time\n",
    "df_analysed = df_analysed.drop(columns = ['internet_speed',\n",
    "                    'max_age_searched',\n",
    "#                     'rental_length_term_>=540days',\n",
    "                    'days_available',\n",
    "                    'min_age_flatmates',\n",
    "                    'min_age_searched'\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629447bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatized Variation Inflation Factor (VIF) analysis\n",
    "# Removing columns must be done one at a time because they influence each others VIF results\n",
    "\n",
    "remove = True\n",
    "cols_to_exclude = []\n",
    "while remove:\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    selected_columns = ['price_euros'] # Ignore the targer column\n",
    "    selected_columns = [col for col in df_analysed.columns.to_list() if col not in selected_columns]\n",
    "\n",
    "    df[\"features\"] = selected_columns\n",
    "\n",
    "    df[\"vif_index\"] = [vif(df_analysed[selected_columns].values, i) for i in range(df_analysed[selected_columns].shape[1])]\n",
    "\n",
    "    df = round(df.sort_values(by=\"vif_index\", ascending = False),2)\n",
    "    \n",
    "    df = df.head(1)\n",
    "\n",
    "    if float(df.vif_index) >= 10:\n",
    "        print(df)\n",
    "        cols_to_exclude = cols_to_exclude + df.features.to_list()\n",
    "        df_analysed = df_analysed.drop(columns = df.features)\n",
    "    else:\n",
    "        remove = False\n",
    "\n",
    "cols_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbb51d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variation Inflation Factor (VIF) analysis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "selected_columns = [col for col in df_analysed.columns.to_list() if col not in ['price_euros']]\n",
    "\n",
    "df[\"features\"] = selected_columns\n",
    "\n",
    "df[\"vif_index\"] = [vif(df_analysed[selected_columns].values, i) for i in range(df_analysed[selected_columns].shape[1])]\n",
    "\n",
    "round(df.sort_values(by=\"vif_index\", ascending = False),2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464dcb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_analysed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d54309",
   "metadata": {},
   "source": [
    "# Feature permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d2f3b",
   "metadata": {},
   "source": [
    "## Permutation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "model = LinearRegression().fit(X, y) # Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_score = permutation_importance(model, X, y,\n",
    "                                           scoring = ['r2','neg_root_mean_squared_error'],\n",
    "                                           n_repeats=100, n_jobs=-1) # Perform Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859e520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(np.vstack((X.columns,\n",
    "                                        permutation_score['r2'].importances_mean,\n",
    "                                       permutation_score['r2'].importances_std,\n",
    "                                        permutation_score['neg_root_mean_squared_error'].importances_mean,\n",
    "                                       permutation_score['neg_root_mean_squared_error'].importances_std)).T) # Unstack results\n",
    "\n",
    "importance_df.columns=['feature',\n",
    "                       'r2 feature importance','r2 feature importance std',\n",
    "                       'RMSE feature importance','RMSE feature importance std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6dee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_df = importance_df.sort_values(by=\"r2 feature importance\", ascending = False) # Order by importance\n",
    "importance_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb614758",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = []\n",
    "scores = []\n",
    "\n",
    "for features in range(1, len(importance_df)): # Loop over the total number of features\n",
    "    \n",
    "    most_important_features = list(importance_df.head(features).feature) # List the name of the features in specific loop\n",
    "   \n",
    "    X_reduced = X[most_important_features] # Make feature set with the selected features\n",
    "    \n",
    "    cv_results = cross_val_score(model, X_reduced, y, cv=10) # cross validate\n",
    "    \n",
    "    scores.append(cv_results.mean()) # Append scores\n",
    "    \n",
    "    top_features.append(features)  # Append number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761bf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(top_features, scores)\n",
    "plt.title('Top features used for modelling vs Scores')\n",
    "# plt.xlim([0,100])\n",
    "plt.ylim([-1,1])\n",
    "plt.xlabel('Top features')\n",
    "plt.ylabel('R2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c83a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importance_df_selected = importance_df[importance_df['r2 feature importance']>= 0.0001]\n",
    "# max_score_n_features = scores.index(max(scores))\n",
    "# importance_df_selected = importance_df.head(max_score_n_features)\n",
    "print(importance_df_selected.shape)\n",
    "importance_df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = importance_df_selected.feature.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfe52a",
   "metadata": {},
   "source": [
    "# Predictive Power Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115257ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correlation analysis that detects assimetric, also non-linear and numeric plus categorical relationships\n",
    "# assimetric: ZIP predicts city but the city is a poor predictor of ZIP code\n",
    "# also non-linear: uses Decision Tree to find relationships that might be linear or not\n",
    "# numeric plus categorical: finds relationships also in categorical features\n",
    "# https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e633a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pps_matrix = pps.matrix(df_analysed[important_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e58254",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style = \"whitegrid\", font_scale= 1)\n",
    "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(25, 35))\n",
    "\n",
    "matrix_df = pps_matrix[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n",
    "sns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=False, annot_kws={\"size\": 8});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5f45a",
   "metadata": {},
   "source": [
    "#  Model parametrization and Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f678ef",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05e9e1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = LinearRegression(n_jobs=-1),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92940e5b",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878af8e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Ridge linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550f43b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b009584",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'alpha': [0.1,1,10,100,1000],\n",
    "    'tol': [0, 0.001,0.1,1],\n",
    "    'solver': ['lsqr']# auto, 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "ridge_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "ridge_rsearch.fit(X,y)\n",
    "print(ridge_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e5479",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = Ridge(alpha = ridge_rsearch.best_params_['alpha'],\n",
    "                                                                          tol = ridge_rsearch.best_params_['tol'],\n",
    "                                                                          solver = ridge_rsearch.best_params_['solver']),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01947d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e38baf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lasso linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca13d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec2ba1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = Lasso()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'alpha': [0.1,1,10,100,1000],\n",
    "    'tol': [0.1,1,10,100,1000],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "lasso_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "lasso_rsearch.fit(X,y)\n",
    "print(lasso_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4793612",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = Lasso(alpha = lasso_rsearch.best_params_['alpha'],\n",
    "                                                                          tol = lasso_rsearch.best_params_['tol'],\n",
    "                                                                          selection = lasso_rsearch.best_params_['selection']),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cd19e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a036f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## ElasticNet linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce37ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed652f0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = ElasticNet()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'alpha': [0.01,0.1,1,10,100],\n",
    "    'tol': [0.1,1,10,100],\n",
    "    'l1_ratio': [0,0.3,0.6,1],\n",
    "    'selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "elastic_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "elastic_rsearch.fit(X,y)\n",
    "print(elastic_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432f3e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = ElasticNet(alpha = elastic_rsearch.best_params_['alpha'],\n",
    "                                                                               l1_ratio = elastic_rsearch.best_params_['l1_ratio'],\n",
    "                                                                          tol = elastic_rsearch.best_params_['tol'],\n",
    "                                                                          selection = elastic_rsearch.best_params_['selection']),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6e0db2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf01df",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stochastic Gradient Descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb135ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae326d7f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'loss':['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'alpha': [0.01,0.1,1],\n",
    "    'penalty': ['elasticnet'],#['l1','l2','elasticnet'],\n",
    "    'tol': [1,10,100],\n",
    "    'l1_ratio': [0,0.3,0.6,1],\n",
    "    'epsilon': [10,100,1000],\n",
    "    'learning_rate': ['invscaling'],#,'constant','optimal','adaptive'],\n",
    "    'eta0': [0.1], \n",
    "    'power_t': [0.25],\n",
    "    'early_stopping': [True]\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "sgdr_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "sgdr_rsearch.fit(X,y)\n",
    "print(sgdr_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5390588",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = SGDRegressor(loss=sgdr_rsearch.best_params_['loss'],\n",
    "                                                                                 penalty=sgdr_rsearch.best_params_['penalty'],\n",
    "                                                                                 alpha=sgdr_rsearch.best_params_['alpha'],\n",
    "                                                                                 l1_ratio=sgdr_rsearch.best_params_['l1_ratio'],\n",
    "                                                                                 fit_intercept=True,\n",
    "                                                                                 max_iter=1000,\n",
    "                                                                                 tol=sgdr_rsearch.best_params_['tol'], \n",
    "                                                                                 shuffle=True, \n",
    "                                                                                 verbose=0, \n",
    "                                                                                 epsilon=sgdr_rsearch.best_params_['epsilon'], \n",
    "                                                                                 random_state=None, \n",
    "                                                                                 learning_rate=sgdr_rsearch.best_params_['learning_rate'],\n",
    "                                                                                 eta0=sgdr_rsearch.best_params_['eta0'], \n",
    "                                                                                 power_t=sgdr_rsearch.best_params_['power_t'], \n",
    "                                                                                 early_stopping=sgdr_rsearch.best_params_['early_stopping'], \n",
    "                                                                                 validation_fraction=0.1,\n",
    "                                                                                 n_iter_no_change=5,\n",
    "                                                                                 warm_start=False, \n",
    "                                                                                 average=False),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c811f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7392c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09aa30",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = KNeighborsRegressor(n_jobs=-1)\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'n_neighbors': range(5,50,5),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': range(10,40,10)\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "knn_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "knn_rsearch.fit(X,y)\n",
    "print(knn_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9f0a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = KNeighborsRegressor(n_neighbors=knn_rsearch.best_params_['n_neighbors'],\n",
    "                                                                                       weights=knn_rsearch.best_params_['weights'],\n",
    "                                                                                       algorithm=knn_rsearch.best_params_['algorithm'],\n",
    "                                                                                       leaf_size=knn_rsearch.best_params_['leaf_size']),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109416b2",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac59c2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44061e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instanciate model\n",
    "model = SVR()\n",
    "\n",
    "# Hyperparameter search space\n",
    "search_space = {\n",
    "    'kernel': ['rbf'],#['linear','poly','sigmoid', 'rbf'],\n",
    "    'degree': [2,3],\n",
    "    'C': [500],#[10,100,1000],\n",
    "    'tol': [0.001,0.01,0.1],\n",
    "    'gamma': [0.1],#[0,0.1,1,'scale','auto'],\n",
    "    'coef0': [0],#[0,0.1,1],\n",
    "    'epsilon': [0.1,1,10]\n",
    "}\n",
    "\n",
    "# Instanciate GridSearchCV\n",
    "svm_rsearch = GridSearchCV(\n",
    "    model, search_space,\n",
    "    n_jobs=-1, scoring='neg_root_mean_squared_error', cv=5, verbose=0)\n",
    "\n",
    "\n",
    "svm_rsearch.fit(X,y)\n",
    "print(svm_rsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35524b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df_analysed.drop(columns=['price_euros'])\n",
    "X = df_analysed[important_features]\n",
    "y = df_analysed['price_euros']\n",
    "\n",
    "# Range for training with 10 equally devided points\n",
    "train_sizes_range = range(int(round(len(y)/10,-2)), \n",
    "                          len(y)-int(round(len(y)/10,-2)),\n",
    "                          int(round(len(y)/10,-2)))\n",
    "\n",
    "# Get train scores, train sizes, and validation scores using `learning_curve`, r2 score\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = SVR(kernel=svm_rsearch.best_params_['kernel'],\n",
    "                                                                        degree=svm_rsearch.best_params_['degree'],\n",
    "                                                                        gamma=svm_rsearch.best_params_['gamma'], \n",
    "                                                                        coef0=svm_rsearch.best_params_['coef0'], \n",
    "                                                                        tol=svm_rsearch.best_params_['tol'],\n",
    "                                                                        C=svm_rsearch.best_params_['C'], \n",
    "                                                                        epsilon=svm_rsearch.best_params_['epsilon'], \n",
    "                                                                        shrinking=True,\n",
    "                                                                        cache_size=200,\n",
    "                                                                        verbose=False,\n",
    "                                                                        max_iter=-1),\n",
    "                                                              X = X, \n",
    "                                                              y = y, \n",
    "                                                              train_sizes = train_sizes_range, \n",
    "                                                              cv = 10,\n",
    "                                                              scoring = 'neg_root_mean_squared_error')\n",
    "\n",
    "# Take the mean of cross-validated train scores and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944893f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "\n",
    "with plt.style.context('seaborn-deep'):\n",
    "    # figsize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # getting axes\n",
    "    ax = plt.gca()\n",
    "    # plotting\n",
    "    ax.plot(train_sizes, train_scores_mean, label = 'Train score',color='blue', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "    ax.plot(train_sizes, test_scores_mean, label = 'Test score',color='orange', linestyle='dashed', marker='o',markerfacecolor='#ffc125', markersize=10)\n",
    "    # more\n",
    "    ax.set_title('Learning Curves', fontsize = 18)\n",
    "    ax.set_xlabel('Training Size', fontsize = 14)\n",
    "    ax.set_ylabel('RMSE', fontsize = 14)\n",
    "    ax.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax.grid(axis=\"y\",linewidth=0.5)\n",
    "    ax.legend(loc=\"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43238af9",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After all this analysis, the linearRegression model seems to outperform all other models....\n",
    "## Also, analysis of the learning curves indicate that the model training is not yet saturated by number of data points, so I'd use as many as possible for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_analysed[important_features]\n",
    "# X = df_analysed.drop(columns=['price_euros'])\n",
    "\n",
    "## Predicting the log of the prices instead of the prices directly improves prediction slightly\n",
    "y = np.log2(df_analysed['price_euros'])\n",
    "\n",
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# INSTANTIATING THE MODEL\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# model = Ridge(alpha = ridge_rsearch.best_params_['alpha'],\n",
    "#               tol = ridge_rsearch.best_params_['tol'],\n",
    "#               solver = ridge_rsearch.best_params_['solver'])\n",
    "\n",
    "# model = Lasso(alpha = lasso_rsearch.best_params_['alpha'],\n",
    "#               tol = lasso_rsearch.best_params_['tol'],\n",
    "#               selection = lasso_rsearch.best_params_['selection'])\n",
    "\n",
    "# model = ElasticNet(alpha = elastic_rsearch.best_params_['alpha'],\n",
    "#                     l1_ratio = elastic_rsearch.best_params_['l1_ratio'],\n",
    "#                     tol = elastic_rsearch.best_params_['tol'],\n",
    "#                     selection = elastic_rsearch.best_params_['selection'])\n",
    "\n",
    "# model = SGDRegressor(loss=sgdr_rsearch.best_params_['loss'],\n",
    "#                      penalty=sgdr_rsearch.best_params_['penalty'],\n",
    "#                      alpha=sgdr_rsearch.best_params_['alpha'],\n",
    "#                      l1_ratio=sgdr_rsearch.best_params_['l1_ratio'],\n",
    "#                      fit_intercept=True,\n",
    "#                      max_iter=1000,\n",
    "#                      tol=sgdr_rsearch.best_params_['tol'], \n",
    "#                      shuffle=True, \n",
    "#                      verbose=0, \n",
    "#                      epsilon=sgdr_rsearch.best_params_['epsilon'], \n",
    "#                      random_state=None, \n",
    "#                      learning_rate=sgdr_rsearch.best_params_['learning_rate'],\n",
    "#                      eta0=sgdr_rsearch.best_params_['eta0'], \n",
    "#                      power_t=sgdr_rsearch.best_params_['power_t'], \n",
    "#                      early_stopping=sgdr_rsearch.best_params_['early_stopping'], \n",
    "#                      validation_fraction=0.1,\n",
    "#                      n_iter_no_change=5,\n",
    "#                      warm_start=False, \n",
    "#                      average=False)\n",
    "\n",
    "# model = KNeighborsRegressor(n_neighbors=knn_rsearch.best_params_['n_neighbors'],\n",
    "#                             weights=knn_rsearch.best_params_['weights'],\n",
    "#                             algorithm=knn_rsearch.best_params_['algorithm'],\n",
    "#                             leaf_size=knn_rsearch.best_params_['leaf_size'])\n",
    "\n",
    "# model = SVR(kernel=svm_rsearch.best_params_['kernel'],\n",
    "#             degree=svm_rsearch.best_params_['degree'],\n",
    "#             gamma=svm_rsearch.best_params_['gamma'],\n",
    "#             coef0=svm_rsearch.best_params_['coef0'], \n",
    "#             tol=svm_rsearch.best_params_['tol'], \n",
    "#             C=svm_rsearch.best_params_['C'], \n",
    "#             epsilon=svm_rsearch.best_params_['epsilon'], \n",
    "#             shrinking=True,\n",
    "#             cache_size=200,\n",
    "#             verbose=False,\n",
    "#             max_iter=-1)\n",
    "\n",
    "# TRAINING THE MODEL ON THE TRAINING SET\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# EVALUATION\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b449bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results(y_true = 2**y_test, y_pred = 2**model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b564d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "y_pred = 2**model.predict(X_test)\n",
    "y_base = [2**np.mean(y)]*len(y_pred)\n",
    "y_true = 2**y_test\n",
    "\n",
    "print(f'RMSE: {np.sqrt(((y_pred - y_true) ** 2).mean())}')\n",
    "print(f'RMSE base model (mean value): {np.sqrt(((y_base - y_true) ** 2).mean())}')\n",
    "\n",
    "n=len(y_pred)\n",
    "residuals = y_true - y_pred\n",
    "residuals_base = y_base - y_pred\n",
    "\n",
    "# residuals.std()/orders.delay_vs_expected.std() * 1/(n**0.5)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10, 5))\n",
    "sns.histplot(residuals, kde=True, edgecolor='w', ax=ax1).set(title='My model')\n",
    "sns.histplot(residuals_base, kde=True, edgecolor='w', ax=ax2).set(title='Mean model')\n",
    "\n",
    "# https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot\n",
    "sm.qqplot(residuals, ax=ax3)\n",
    "sm.qqplot(residuals_base, ax=ax4)\n",
    "plt.tight_layout()\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5796c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_pred, y=residuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7db8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "458.97px",
    "width": "346.968px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
